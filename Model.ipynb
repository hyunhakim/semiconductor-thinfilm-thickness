{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\Users\\student\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\__init__.py:15: FutureWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import datasets, preprocessing\n",
    "from sklearn import model_selection\n",
    "from keras import datasets, utils\n",
    "from keras import models, layers, activations, initializers, losses, optimizers, metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "import os\n",
    "import warnings\n",
    "from sklearn.externals import joblib\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data의 각 열들의 correlation\n",
    "corr = train.corr()\n",
    "\n",
    "# layer_1과 correlation 높은 columns 추출\n",
    "corr_1 = corr[np.abs(corr['layer_1']) > 0.03]['layer_1'][1:]\n",
    "cor_col_1 = np.array(pd.DataFrame(corr_1).index)\n",
    "\n",
    "# layer_2과 correlation 높은 columns 추출\n",
    "corr_2 = corr[np.abs(corr['layer_2']) > 0.01]['layer_2'][1:]\n",
    "cor_col_2 = np.array(pd.DataFrame(corr_2).index)\n",
    "\n",
    "# layer_3과 correlation 높은 columns 추출\n",
    "corr_3 = corr[np.abs(corr['layer_3']) > 0.03]['layer_3'][1:]\n",
    "cor_col_3 = np.array(pd.DataFrame(corr_3).index)\n",
    "\n",
    "# layer_4과 correlation 높은 columns 추출\n",
    "corr_4 = corr[np.abs(corr['layer_4']) > 0.04]['layer_4'][1:]\n",
    "cor_col_4 = np.array(pd.DataFrame(corr_4).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1의 train data 추출\n",
    "train_layer_1 = train[cor_col_1]\n",
    "train_layer_1_target = train['layer_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "# layer 1 예측 모델\n",
    "model1 = Sequential()\n",
    "model1.add(Dense(units=160, activation='relu', input_dim=len(cor_col_1)))\n",
    "model1.add(Dense(units=160, activation='relu'))\n",
    "model1.add(Dense(units=160, activation='relu'))\n",
    "model1.add(Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\student\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 769500 samples, validate on 40500 samples\n",
      "Epoch 1/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 56.9435 - mae: 56.9434 - val_loss: 96.5861 - val_mae: 96.5861\n",
      "Epoch 2/20\n",
      "769500/769500 [==============================] - 11s 15us/step - loss: 47.7604 - mae: 47.7604 - val_loss: 79.3467 - val_mae: 79.3467\n",
      "Epoch 3/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 44.1336 - mae: 44.1336 - val_loss: 98.4425 - val_mae: 98.4425\n",
      "Epoch 4/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 41.9536 - mae: 41.9537 - val_loss: 96.8739 - val_mae: 96.8739\n",
      "Epoch 5/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 40.0912 - mae: 40.0911 - val_loss: 75.8923 - val_mae: 75.8923\n",
      "Epoch 6/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 38.4895 - mae: 38.4894 - val_loss: 90.0675 - val_mae: 90.0675\n",
      "Epoch 7/20\n",
      "769500/769500 [==============================] - 10s 13us/step - loss: 36.9271 - mae: 36.9271 - val_loss: 86.2787 - val_mae: 86.2787\n",
      "Epoch 8/20\n",
      "769500/769500 [==============================] - 11s 15us/step - loss: 35.5421 - mae: 35.5420 - val_loss: 88.0867 - val_mae: 88.0867\n",
      "Epoch 9/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 34.1463 - mae: 34.1463 - val_loss: 81.0367 - val_mae: 81.0368\n",
      "Epoch 10/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 33.0208 - mae: 33.0209 - val_loss: 89.9531 - val_mae: 89.9531\n",
      "Epoch 11/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 32.1347 - mae: 32.1347 - val_loss: 84.2303 - val_mae: 84.2302\n",
      "Epoch 12/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 31.1993 - mae: 31.1993 - val_loss: 81.4871 - val_mae: 81.4872\n",
      "Epoch 13/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 30.5433 - mae: 30.5434 - val_loss: 86.8340 - val_mae: 86.8340\n",
      "Epoch 14/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 29.8617 - mae: 29.8617 - val_loss: 74.5896 - val_mae: 74.5896\n",
      "Epoch 15/20\n",
      "769500/769500 [==============================] - 11s 15us/step - loss: 29.2584 - mae: 29.2584 - val_loss: 77.7532 - val_mae: 77.7533\n",
      "Epoch 16/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 28.6814 - mae: 28.6814 - val_loss: 82.1471 - val_mae: 82.1471\n",
      "Epoch 17/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 28.0922 - mae: 28.0922 - val_loss: 80.8327 - val_mae: 80.8327\n",
      "Epoch 18/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 27.6465 - mae: 27.6465 - val_loss: 83.0357 - val_mae: 83.0358\n",
      "Epoch 19/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 27.1638 - mae: 27.1637 - val_loss: 77.9947 - val_mae: 77.9947\n",
      "Epoch 20/20\n",
      "769500/769500 [==============================] - 10s 14us/step - loss: 26.7684 - mae: 26.7684 - val_loss: 74.3523 - val_mae: 74.3523\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x219a0c30588>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1.fit(train_layer_1, train_layer_1_target, epochs=20, batch_size=128, validation_split = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>...</th>\n",
       "      <th>216</th>\n",
       "      <th>217</th>\n",
       "      <th>218</th>\n",
       "      <th>219</th>\n",
       "      <th>220</th>\n",
       "      <th>221</th>\n",
       "      <th>222</th>\n",
       "      <th>223</th>\n",
       "      <th>224</th>\n",
       "      <th>225</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.535410</td>\n",
       "      <td>0.520775</td>\n",
       "      <td>0.494087</td>\n",
       "      <td>0.465134</td>\n",
       "      <td>0.430339</td>\n",
       "      <td>0.401751</td>\n",
       "      <td>0.355986</td>\n",
       "      <td>0.326427</td>\n",
       "      <td>0.282340</td>\n",
       "      <td>...</td>\n",
       "      <td>0.748339</td>\n",
       "      <td>0.757575</td>\n",
       "      <td>0.768130</td>\n",
       "      <td>0.777062</td>\n",
       "      <td>0.769173</td>\n",
       "      <td>0.768253</td>\n",
       "      <td>0.738704</td>\n",
       "      <td>0.739460</td>\n",
       "      <td>0.702139</td>\n",
       "      <td>0.702238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.351099</td>\n",
       "      <td>0.398179</td>\n",
       "      <td>0.413809</td>\n",
       "      <td>0.418529</td>\n",
       "      <td>0.433257</td>\n",
       "      <td>0.455410</td>\n",
       "      <td>0.451065</td>\n",
       "      <td>0.464230</td>\n",
       "      <td>0.476011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333931</td>\n",
       "      <td>0.276307</td>\n",
       "      <td>0.211513</td>\n",
       "      <td>0.159223</td>\n",
       "      <td>0.110982</td>\n",
       "      <td>0.083130</td>\n",
       "      <td>0.099780</td>\n",
       "      <td>0.145420</td>\n",
       "      <td>0.260501</td>\n",
       "      <td>0.343857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.490537</td>\n",
       "      <td>0.435958</td>\n",
       "      <td>0.413428</td>\n",
       "      <td>0.355796</td>\n",
       "      <td>0.335777</td>\n",
       "      <td>0.299944</td>\n",
       "      <td>0.242745</td>\n",
       "      <td>0.210555</td>\n",
       "      <td>0.180739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.709371</td>\n",
       "      <td>0.746826</td>\n",
       "      <td>0.781436</td>\n",
       "      <td>0.788292</td>\n",
       "      <td>0.828630</td>\n",
       "      <td>0.835166</td>\n",
       "      <td>0.845859</td>\n",
       "      <td>0.846032</td>\n",
       "      <td>0.836724</td>\n",
       "      <td>0.846779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.051634</td>\n",
       "      <td>0.075802</td>\n",
       "      <td>0.133983</td>\n",
       "      <td>0.154546</td>\n",
       "      <td>0.209387</td>\n",
       "      <td>0.251700</td>\n",
       "      <td>0.287552</td>\n",
       "      <td>0.333000</td>\n",
       "      <td>0.340617</td>\n",
       "      <td>...</td>\n",
       "      <td>0.075046</td>\n",
       "      <td>0.056651</td>\n",
       "      <td>0.079884</td>\n",
       "      <td>0.147469</td>\n",
       "      <td>0.213112</td>\n",
       "      <td>0.298096</td>\n",
       "      <td>0.382823</td>\n",
       "      <td>0.489381</td>\n",
       "      <td>0.562383</td>\n",
       "      <td>0.599247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.154031</td>\n",
       "      <td>0.201728</td>\n",
       "      <td>0.270414</td>\n",
       "      <td>0.283799</td>\n",
       "      <td>0.343050</td>\n",
       "      <td>0.340233</td>\n",
       "      <td>0.379244</td>\n",
       "      <td>0.378511</td>\n",
       "      <td>0.373017</td>\n",
       "      <td>...</td>\n",
       "      <td>0.255070</td>\n",
       "      <td>0.242396</td>\n",
       "      <td>0.271287</td>\n",
       "      <td>0.328828</td>\n",
       "      <td>0.397950</td>\n",
       "      <td>0.486436</td>\n",
       "      <td>0.530573</td>\n",
       "      <td>0.582752</td>\n",
       "      <td>0.637296</td>\n",
       "      <td>0.637238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9995</th>\n",
       "      <td>9995</td>\n",
       "      <td>0.199957</td>\n",
       "      <td>0.227188</td>\n",
       "      <td>0.250628</td>\n",
       "      <td>0.265388</td>\n",
       "      <td>0.291736</td>\n",
       "      <td>0.319845</td>\n",
       "      <td>0.339820</td>\n",
       "      <td>0.368420</td>\n",
       "      <td>0.373319</td>\n",
       "      <td>...</td>\n",
       "      <td>0.422622</td>\n",
       "      <td>0.410483</td>\n",
       "      <td>0.409814</td>\n",
       "      <td>0.420162</td>\n",
       "      <td>0.426533</td>\n",
       "      <td>0.445706</td>\n",
       "      <td>0.487397</td>\n",
       "      <td>0.495991</td>\n",
       "      <td>0.534095</td>\n",
       "      <td>0.549278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9996</th>\n",
       "      <td>9996</td>\n",
       "      <td>0.180469</td>\n",
       "      <td>0.142472</td>\n",
       "      <td>0.112432</td>\n",
       "      <td>0.084084</td>\n",
       "      <td>0.071504</td>\n",
       "      <td>0.068979</td>\n",
       "      <td>0.070754</td>\n",
       "      <td>0.063943</td>\n",
       "      <td>0.071106</td>\n",
       "      <td>...</td>\n",
       "      <td>0.745875</td>\n",
       "      <td>0.776221</td>\n",
       "      <td>0.767607</td>\n",
       "      <td>0.774457</td>\n",
       "      <td>0.805273</td>\n",
       "      <td>0.802651</td>\n",
       "      <td>0.810866</td>\n",
       "      <td>0.792099</td>\n",
       "      <td>0.796827</td>\n",
       "      <td>0.791949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9997</th>\n",
       "      <td>9997</td>\n",
       "      <td>0.169476</td>\n",
       "      <td>0.180695</td>\n",
       "      <td>0.225148</td>\n",
       "      <td>0.220553</td>\n",
       "      <td>0.262136</td>\n",
       "      <td>0.288092</td>\n",
       "      <td>0.280675</td>\n",
       "      <td>0.312065</td>\n",
       "      <td>0.304840</td>\n",
       "      <td>...</td>\n",
       "      <td>0.282820</td>\n",
       "      <td>0.296270</td>\n",
       "      <td>0.324376</td>\n",
       "      <td>0.391588</td>\n",
       "      <td>0.436017</td>\n",
       "      <td>0.500170</td>\n",
       "      <td>0.569207</td>\n",
       "      <td>0.623997</td>\n",
       "      <td>0.673445</td>\n",
       "      <td>0.688012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9998</th>\n",
       "      <td>9998</td>\n",
       "      <td>0.218762</td>\n",
       "      <td>0.204023</td>\n",
       "      <td>0.207701</td>\n",
       "      <td>0.198991</td>\n",
       "      <td>0.188334</td>\n",
       "      <td>0.173722</td>\n",
       "      <td>0.161461</td>\n",
       "      <td>0.155859</td>\n",
       "      <td>0.136998</td>\n",
       "      <td>...</td>\n",
       "      <td>0.650535</td>\n",
       "      <td>0.662007</td>\n",
       "      <td>0.688480</td>\n",
       "      <td>0.708460</td>\n",
       "      <td>0.722464</td>\n",
       "      <td>0.726888</td>\n",
       "      <td>0.758949</td>\n",
       "      <td>0.771153</td>\n",
       "      <td>0.769234</td>\n",
       "      <td>0.785455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9999</th>\n",
       "      <td>9999</td>\n",
       "      <td>0.383608</td>\n",
       "      <td>0.355178</td>\n",
       "      <td>0.347031</td>\n",
       "      <td>0.338729</td>\n",
       "      <td>0.315327</td>\n",
       "      <td>0.316411</td>\n",
       "      <td>0.318317</td>\n",
       "      <td>0.285892</td>\n",
       "      <td>0.275320</td>\n",
       "      <td>...</td>\n",
       "      <td>0.321021</td>\n",
       "      <td>0.335527</td>\n",
       "      <td>0.339900</td>\n",
       "      <td>0.352514</td>\n",
       "      <td>0.388642</td>\n",
       "      <td>0.390270</td>\n",
       "      <td>0.406929</td>\n",
       "      <td>0.446899</td>\n",
       "      <td>0.451189</td>\n",
       "      <td>0.472153</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10000 rows × 227 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id         0         1         2         3         4         5  \\\n",
       "0        0  0.535410  0.520775  0.494087  0.465134  0.430339  0.401751   \n",
       "1        1  0.351099  0.398179  0.413809  0.418529  0.433257  0.455410   \n",
       "2        2  0.490537  0.435958  0.413428  0.355796  0.335777  0.299944   \n",
       "3        3  0.051634  0.075802  0.133983  0.154546  0.209387  0.251700   \n",
       "4        4  0.154031  0.201728  0.270414  0.283799  0.343050  0.340233   \n",
       "...    ...       ...       ...       ...       ...       ...       ...   \n",
       "9995  9995  0.199957  0.227188  0.250628  0.265388  0.291736  0.319845   \n",
       "9996  9996  0.180469  0.142472  0.112432  0.084084  0.071504  0.068979   \n",
       "9997  9997  0.169476  0.180695  0.225148  0.220553  0.262136  0.288092   \n",
       "9998  9998  0.218762  0.204023  0.207701  0.198991  0.188334  0.173722   \n",
       "9999  9999  0.383608  0.355178  0.347031  0.338729  0.315327  0.316411   \n",
       "\n",
       "             6         7         8  ...       216       217       218  \\\n",
       "0     0.355986  0.326427  0.282340  ...  0.748339  0.757575  0.768130   \n",
       "1     0.451065  0.464230  0.476011  ...  0.333931  0.276307  0.211513   \n",
       "2     0.242745  0.210555  0.180739  ...  0.709371  0.746826  0.781436   \n",
       "3     0.287552  0.333000  0.340617  ...  0.075046  0.056651  0.079884   \n",
       "4     0.379244  0.378511  0.373017  ...  0.255070  0.242396  0.271287   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "9995  0.339820  0.368420  0.373319  ...  0.422622  0.410483  0.409814   \n",
       "9996  0.070754  0.063943  0.071106  ...  0.745875  0.776221  0.767607   \n",
       "9997  0.280675  0.312065  0.304840  ...  0.282820  0.296270  0.324376   \n",
       "9998  0.161461  0.155859  0.136998  ...  0.650535  0.662007  0.688480   \n",
       "9999  0.318317  0.285892  0.275320  ...  0.321021  0.335527  0.339900   \n",
       "\n",
       "           219       220       221       222       223       224       225  \n",
       "0     0.777062  0.769173  0.768253  0.738704  0.739460  0.702139  0.702238  \n",
       "1     0.159223  0.110982  0.083130  0.099780  0.145420  0.260501  0.343857  \n",
       "2     0.788292  0.828630  0.835166  0.845859  0.846032  0.836724  0.846779  \n",
       "3     0.147469  0.213112  0.298096  0.382823  0.489381  0.562383  0.599247  \n",
       "4     0.328828  0.397950  0.486436  0.530573  0.582752  0.637296  0.637238  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "9995  0.420162  0.426533  0.445706  0.487397  0.495991  0.534095  0.549278  \n",
       "9996  0.774457  0.805273  0.802651  0.810866  0.792099  0.796827  0.791949  \n",
       "9997  0.391588  0.436017  0.500170  0.569207  0.623997  0.673445  0.688012  \n",
       "9998  0.708460  0.722464  0.726888  0.758949  0.771153  0.769234  0.785455  \n",
       "9999  0.352514  0.388642  0.390270  0.406929  0.446899  0.451189  0.472153  \n",
       "\n",
       "[10000 rows x 227 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 1 예측 모델 저장\n",
    "joblib.dump(model1, 'model/model1_layer1.pkl', compress=True)\n",
    "\n",
    "# layer 1 예측\n",
    "pred_test_layer_1 = model1.predict(test[cor_col_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음수는 1.0 대입\n",
    "pred_test_layer_1[pred_test_layer_1 < 0] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 2의 train data 추출\n",
    "train_layer_2 = train[cor_col_2]\n",
    "train_layer_2_target = train['layer_2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 2 예측 모델\n",
    "model2 = Sequential()\n",
    "model2.add(Dense(units=160, activation='relu', input_dim=len(cor_col_2)))\n",
    "model2.add(Dense(units=160, activation='relu'))\n",
    "model2.add(Dense(units=160, activation='relu'))\n",
    "model2.add(Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769500 samples, validate on 40500 samples\n",
      "Epoch 1/20\n",
      "769500/769500 [==============================] - 10s 14us/step - loss: 68.8613 - mae: 68.8613 - val_loss: 65.7740 - val_mae: 65.7740\n",
      "Epoch 2/20\n",
      "769500/769500 [==============================] - 10s 13us/step - loss: 66.6505 - mae: 66.6505 - val_loss: 63.9771 - val_mae: 63.9771\n",
      "Epoch 3/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 66.1360 - mae: 66.1360 - val_loss: 62.3511 - val_mae: 62.3511\n",
      "Epoch 4/20\n",
      "769500/769500 [==============================] - 10s 14us/step - loss: 65.8579 - mae: 65.8580 - val_loss: 61.8666 - val_mae: 61.8667\n",
      "Epoch 5/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 65.6310 - mae: 65.6309 - val_loss: 62.6421 - val_mae: 62.6421\n",
      "Epoch 6/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 65.4026 - mae: 65.4027 - val_loss: 61.5177 - val_mae: 61.5177\n",
      "Epoch 7/20\n",
      "769500/769500 [==============================] - 10s 13us/step - loss: 65.1189 - mae: 65.1189 - val_loss: 62.0516 - val_mae: 62.0516\n",
      "Epoch 8/20\n",
      "769500/769500 [==============================] - 10s 13us/step - loss: 64.8253 - mae: 64.8256 - val_loss: 61.0726 - val_mae: 61.0726\n",
      "Epoch 9/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 64.5782 - mae: 64.5782 - val_loss: 60.6603 - val_mae: 60.6603\n",
      "Epoch 10/20\n",
      "769500/769500 [==============================] - 10s 12us/step - loss: 64.3823 - mae: 64.3824 - val_loss: 63.7473 - val_mae: 63.7472\n",
      "Epoch 11/20\n",
      "769500/769500 [==============================] - 10s 13us/step - loss: 64.2164 - mae: 64.2162 - val_loss: 60.6815 - val_mae: 60.6815\n",
      "Epoch 12/20\n",
      "769500/769500 [==============================] - 10s 13us/step - loss: 64.0609 - mae: 64.0609 - val_loss: 60.8374 - val_mae: 60.8374\n",
      "Epoch 13/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 63.9201 - mae: 63.9202 - val_loss: 61.7813 - val_mae: 61.7813\n",
      "Epoch 14/20\n",
      "769500/769500 [==============================] - 11s 15us/step - loss: 63.7629 - mae: 63.7629 - val_loss: 60.9360 - val_mae: 60.9360\n",
      "Epoch 15/20\n",
      "769500/769500 [==============================] - 11s 15us/step - loss: 63.6326 - mae: 63.6326 - val_loss: 61.7592 - val_mae: 61.7591\n",
      "Epoch 16/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 63.4822 - mae: 63.4824 - val_loss: 62.7463 - val_mae: 62.7463\n",
      "Epoch 17/20\n",
      "769500/769500 [==============================] - 10s 14us/step - loss: 63.3215 - mae: 63.3214 - val_loss: 61.8938 - val_mae: 61.8938\n",
      "Epoch 18/20\n",
      "769500/769500 [==============================] - 10s 14us/step - loss: 63.1751 - mae: 63.1750 - val_loss: 64.0877 - val_mae: 64.0877\n",
      "Epoch 19/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 63.0159 - mae: 63.0160 - val_loss: 63.7766 - val_mae: 63.7766\n",
      "Epoch 20/20\n",
      "769500/769500 [==============================] - 10s 13us/step - loss: 62.8635 - mae: 62.8633 - val_loss: 62.7570 - val_mae: 62.7569\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x219a4ed5358>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2.fit(train_layer_2, train_layer_2_target, epochs=20, batch_size=128, validation_split = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 2 예측 모델 저장\n",
    "joblib.dump(model2, 'model/model1_layer2.pkl', compress=True)\n",
    "\n",
    "# layer 2 예측\n",
    "pred_test_layer_2 = model2.predict(test[cor_col_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 음수는 1.0 대입\n",
    "pred_test_layer_2[pred_test_layer_2 < 0] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 3의 train data 추출\n",
    "train_layer_3 = train[cor_col_3]\n",
    "train_layer_3_target = train['layer_3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 3 예측 모델\n",
    "model3 = Sequential()\n",
    "model3.add(Dense(units=160, activation='relu', input_dim=len(cor_col_3)))\n",
    "model3.add(Dense(units=160, activation='relu'))\n",
    "model3.add(Dense(units=160, activation='relu'))\n",
    "model3.add(Dense(units=1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3.compile(loss='mae', optimizer='adam', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769500 samples, validate on 40500 samples\n",
      "Epoch 1/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 58.9274 - mae: 58.9273 - val_loss: 76.4273 - val_mae: 76.4273\n",
      "Epoch 2/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 53.7082 - mae: 53.7082 - val_loss: 80.5723 - val_mae: 80.5723\n",
      "Epoch 3/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 51.5450 - mae: 51.5451 - val_loss: 79.9458 - val_mae: 79.9458\n",
      "Epoch 4/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 49.8408 - mae: 49.8408 - val_loss: 71.3977 - val_mae: 71.3977\n",
      "Epoch 5/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 48.4220 - mae: 48.4221 - val_loss: 69.7526 - val_mae: 69.7526\n",
      "Epoch 6/20\n",
      "769500/769500 [==============================] - 10s 14us/step - loss: 47.2409 - mae: 47.2410 - val_loss: 73.6505 - val_mae: 73.6505\n",
      "Epoch 7/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 46.1965 - mae: 46.1965 - val_loss: 74.2985 - val_mae: 74.2985\n",
      "Epoch 8/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 45.2965 - mae: 45.2965 - val_loss: 66.6634 - val_mae: 66.6634\n",
      "Epoch 9/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 44.4531 - mae: 44.4532 - val_loss: 74.4488 - val_mae: 74.4488\n",
      "Epoch 10/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 43.7113 - mae: 43.7113 - val_loss: 74.0360 - val_mae: 74.0360\n",
      "Epoch 11/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 43.0556 - mae: 43.0556 - val_loss: 72.3534 - val_mae: 72.3534\n",
      "Epoch 12/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 42.4548 - mae: 42.4547 - val_loss: 77.6898 - val_mae: 77.6898\n",
      "Epoch 13/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 41.9286 - mae: 41.9286 - val_loss: 72.9681 - val_mae: 72.9681\n",
      "Epoch 14/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 41.4115 - mae: 41.4116 - val_loss: 67.2436 - val_mae: 67.2436\n",
      "Epoch 15/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 40.9445 - mae: 40.9445 - val_loss: 66.3393 - val_mae: 66.3393\n",
      "Epoch 16/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 40.5249 - mae: 40.5249 - val_loss: 67.9212 - val_mae: 67.9212\n",
      "Epoch 17/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 40.1065 - mae: 40.1064 - val_loss: 68.0811 - val_mae: 68.0811\n",
      "Epoch 18/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 39.6994 - mae: 39.6994 - val_loss: 67.2895 - val_mae: 67.2895\n",
      "Epoch 19/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 39.3295 - mae: 39.3295 - val_loss: 64.2516 - val_mae: 64.2516\n",
      "Epoch 20/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 38.9673 - mae: 38.9673 - val_loss: 64.9793 - val_mae: 64.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x219a57bea58>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model3.fit(train_layer_3, train_layer_3_target, epochs=20, batch_size=128, validation_split = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 3 예측 모델 저장\n",
    "joblib.dump(model3, 'model/model1_layer3.pkl', compress=True)\n",
    "\n",
    "# layer 3 예측\n",
    "pred_test_layer_3 = model3.predict(test[cor_col_3])\n",
    "\n",
    "# 음수는 1.0 대입\n",
    "pred_test_layer_3[pred_test_layer_3 < 0] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 769500 samples, validate on 40500 samples\n",
      "Epoch 1/20\n",
      "769500/769500 [==============================] - 13s 16us/step - loss: 65.6702 - mae: 65.6701 - val_loss: 67.3483 - val_mae: 67.3482\n",
      "Epoch 2/20\n",
      "769500/769500 [==============================] - 13s 16us/step - loss: 60.3656 - mae: 60.3657 - val_loss: 68.5464 - val_mae: 68.5464\n",
      "Epoch 3/20\n",
      "769500/769500 [==============================] - 14s 18us/step - loss: 58.0224 - mae: 58.0224 - val_loss: 66.6321 - val_mae: 66.6321\n",
      "Epoch 4/20\n",
      "769500/769500 [==============================] - 13s 16us/step - loss: 56.2064 - mae: 56.2065 - val_loss: 70.2051 - val_mae: 70.2051\n",
      "Epoch 5/20\n",
      "769500/769500 [==============================] - 13s 16us/step - loss: 54.7549 - mae: 54.7549 - val_loss: 63.7959 - val_mae: 63.7959\n",
      "Epoch 6/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 53.5171 - mae: 53.5170 - val_loss: 61.5281 - val_mae: 61.5281\n",
      "Epoch 7/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 52.5708 - mae: 52.5709 - val_loss: 62.2625 - val_mae: 62.2625\n",
      "Epoch 8/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 51.6767 - mae: 51.6768 - val_loss: 60.7729 - val_mae: 60.7729\n",
      "Epoch 9/20\n",
      "769500/769500 [==============================] - 11s 15us/step - loss: 50.8942 - mae: 50.8942 - val_loss: 63.1171 - val_mae: 63.1171\n",
      "Epoch 10/20\n",
      "769500/769500 [==============================] - 11s 14us/step - loss: 50.2730 - mae: 50.2729 - val_loss: 60.4994 - val_mae: 60.4994\n",
      "Epoch 11/20\n",
      "769500/769500 [==============================] - 12s 16us/step - loss: 49.5939 - mae: 49.5940 - val_loss: 58.6525 - val_mae: 58.6525\n",
      "Epoch 12/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 48.9700 - mae: 48.9700 - val_loss: 63.6813 - val_mae: 63.6813\n",
      "Epoch 13/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 48.4771 - mae: 48.4771 - val_loss: 59.6202 - val_mae: 59.6202\n",
      "Epoch 14/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 47.9599 - mae: 47.9600 - val_loss: 60.7999 - val_mae: 60.7999\n",
      "Epoch 15/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 47.4829 - mae: 47.4829 - val_loss: 60.1056 - val_mae: 60.1056\n",
      "Epoch 16/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 47.0524 - mae: 47.0523 - val_loss: 59.9574 - val_mae: 59.9574\n",
      "Epoch 17/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 46.5957 - mae: 46.5956 - val_loss: 59.0984 - val_mae: 59.0984\n",
      "Epoch 18/20\n",
      "769500/769500 [==============================] - 13s 17us/step - loss: 46.1917 - mae: 46.1917 - val_loss: 56.2125 - val_mae: 56.2125\n",
      "Epoch 19/20\n",
      "769500/769500 [==============================] - 12s 15us/step - loss: 45.8095 - mae: 45.8095 - val_loss: 58.9109 - val_mae: 58.9109\n",
      "Epoch 20/20\n",
      "769500/769500 [==============================] - 13s 16us/step - loss: 45.4284 - mae: 45.4285 - val_loss: 56.5495 - val_mae: 56.5495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x21a37cfdcf8>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# layer 4의 train data 추출\n",
    "train_layer_4 = train[cor_col_4]\n",
    "train_layer_4_target = train['layer_4']\n",
    "\n",
    "# layer 4 예측 모델\n",
    "model4 = Sequential()\n",
    "model4.add(Dense(units=160, activation='relu', input_dim=len(cor_col_4)))\n",
    "model4.add(Dense(units=160, activation='relu'))\n",
    "model4.add(Dense(units=160, activation='relu'))\n",
    "model4.add(Dense(units=1, activation='linear'))\n",
    "\n",
    "model4.compile(loss='mae', optimizer='adam', metrics=['mae'])\n",
    "\n",
    "model4.fit(train_layer_4, train_layer_4_target, epochs=20, batch_size=128, validation_split = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layer 4 예측 모델 저장\n",
    "joblib.dump(model4, 'model/model1_layer4.pkl', compress=True)\n",
    "\n",
    "# layer 4 예측\n",
    "pred_test_layer_4 = model4.predict(test[cor_col_4])\n",
    "\n",
    "# 음수는 1.0 대입\n",
    "pred_test_layer_4[pred_test_layer_4 < 0] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측한 값들 합쳐서 엑셀로 만들기\n",
    "sample_sub = pd.read_csv('data/sample_submission.csv', index_col=0)\n",
    "submission = sample_sub+np.concatenate((pred_test_layer_1, pred_test_layer_2, pred_test_layer_3, pred_test_layer_4), axis=1)\n",
    "submission.to_csv('./test/submission1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memo\n",
    "1. data augmentation -> 여러 모델로 예측하고 그 평균값으로"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
